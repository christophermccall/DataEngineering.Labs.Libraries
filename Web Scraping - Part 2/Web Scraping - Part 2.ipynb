{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Let's find the link for every product page and then extract from it"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#import bs4 (beautifulsoup) and requests\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#store the URL to the main page of the books website:\n",
    "\n",
    "main_page = 'http://books.toscrape.com/catalogue/page-1.html'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#using the requests library do a pull of the main page\n",
    "\n",
    "page = requests.get(main_page).text\n",
    "page"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#parse out the above html using bs4\n",
    "\n",
    "parsed = BeautifulSoup(page)\n",
    "parsed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#make a helper function to do both the 'get' and the parsing all at once\n",
    "\n",
    "def parse(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    return(soup)\n",
    "\n",
    "parse(main_page)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Starting basic. Let's find every link on the page. These are always going to be within the \"< a >\" tags in the page's html. The most important attribute of the tag is what's called the 'href'. The href defines where the link actually goes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#Extract every link on the page. This should be three lines of code, max.\n",
    "\n",
    "soup = parse(main_page)\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ok, so we've found every link, including the ones that link to each book's individual page. The problem is, there are a bunch of other links we don't want to go to. So, let's get a little more granular.\n",
    "\n",
    "By right clicking on a link in your browser, you'll see an option to 'inspect'. Inspect the links for each book - at first glance it seems like every link is inside an 'h3' tag. On more complex websites, you might not get this lucky. Sometimes, you'll notice the links you want are all in one section, are all in a class of the same name, or follow some sort of other pattern. \n",
    "\n",
    "Webscraping is always site specific. Don't get overwhelmed though! It's really just a matter of finding the pattern in the html, so you can programtically identify the stuff you want to scrape!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#make a list of all the h3 sections\n",
    "\n",
    "sections = parse(main_page).find_all('h3')\n",
    "sections"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Seems like we're on the right track. Let's see if we can get the actual links from these h3 sections."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#show the first object in the list\n",
    "\n",
    "sections[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#index into the 'a' tag\n",
    "\n",
    "sections[0].a"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#get just the link\n",
    "\n",
    "sections[0].a['href']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Success! It looks like we've found a way to get the link for each page! Now let's loop through every page and every block that we think has a link. If we do this right, we should get 1000 results."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Write a loop that pulls every link on the first page. You should expect to return 20 unique links. Print the results.\n",
    "\n",
    "links = []\n",
    "page = \"http://books.toscrape.com/catalogue/page-1.html\"\n",
    "soup = parse(page)\n",
    "for x in soup.find_all('h3'):\n",
    "    links.append(x.a['href'])\n",
    "\n",
    "print('Looks like there are {} unique links on {}.\\n\\n\\n'.format(len(set(links)), page))\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Success! We now have 20 links. Next we need to figure out how to do this same thing for all 50 pages of books on the website. Click around a few pages and note what changes in the URL. You should notice only one thing changing!\n",
    "\n",
    "Write a function that loops through each of the 50 pages of the website and uses list comprehension to find each page link.\n",
    "\n",
    "I'll give you a hint here. You'll need to use '.format()' or something similar on the URL, so that you can replace the page number on each new loop. I did it this way:\n",
    "\n",
    "'http://books.toscrape.com/catalogue/page-{}.html'.format()\n",
    "\n",
    "Note the curly brackets after 'page-'. This allows you to pass a number into .format() which will pass itself into those curly brackets."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#write your function here\n",
    "\n",
    "def link_puller(link, number_of_pages):\n",
    "\n",
    "    links = []\n",
    "    for i in range(1,number_of_pages+1):\n",
    "        page = link.format(i)\n",
    "        soup = parse(page)\n",
    "        page_links = [x.a['href'] for x in soup.find_all('h3')]\n",
    "        links.extend(page_links)\n",
    "\n",
    "    print('Found {} unique links.'.format(len(set(links))))\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#run your function against the site and confirm you get 1000 links as expected\n",
    "\n",
    "links = link_puller(link='http://books.toscrape.com/catalogue/page-{}.html', number_of_pages=50)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Congrats. We now have all the links we'll need to scrape every individual page! üòÅ\n",
    "In the next lab, we'll use the links to scrape all the individual data points we're interested in."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#import pickle and save your list of links as a .pkl file for use in the next lab\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('links.pkl', 'wb') as f:\n",
    "    pickle.dump(links, f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#unpickle it to confirm it worked\n",
    "\n",
    "with open('links.pkl', 'rb') as f:\n",
    "    links = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
